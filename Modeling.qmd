---
title: "Modeling Diabetes Incidence"
author: "Cass Crews"
format: html
editor: visual
---

# Introduction

# Loading Packages and Processing the Data

```{r}
library(tidyverse)
library(tidymodels)
library(rsample)
library(ranger)
library(vip)
library(future)
```

```{r}
raw_data<-read_csv("diabetes_binary_health_indicators_BRFSS2015.csv") |>
  select(Diabetes_binary, HighBP, HighChol, PhysActivity, Fruits, Veggies, Smoker, HvyAlcoholConsump, Sex, Age, BMI, Income)

data<-raw_data |>
  mutate(diabetes_binary = factor(Diabetes_binary, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(high_bp = factor(HighBP, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(high_chol = factor(HighChol, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(phys_activity = factor(PhysActivity, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(fruits = factor(Fruits, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(veggies = factor(Veggies, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(smoker = factor(Smoker, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(hvy_alc_consump = factor(HvyAlcoholConsump, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(sex = factor(Sex, levels = 0:1, labels = c("Female", "Male"))) |>
  mutate(age = factor(Age, levels = 1:13, labels = c("18 to 24", "25 to 29",
                                                     "30 to 34", "35 to 39",
                                                     "40 to 44", "45 to 49",
                                                     "50 to 54", "55 to 59",
                                                     "60 to 64", "65 to 69",
                                                     "70 to 74", "75 to 79",
                                                     "80+"))) |> 
  mutate(income = factor(Income, levels = 1:8, labels = c("Less than $10k", "$10k to $15k",
                                                     "$15k to $20k", "$20k to $25k",
                                                     "$25k to $35k", "$35k to $50k",
                                                     "$50k to $75k", "$75k+"))) |> 
  rename(bmi = BMI) |>
  select(!c(Diabetes_binary:Age, Income))
```

# Preparing the Data for Model Tuning

```{r}
#Utilizing parallel processing with 5 cores
plan(multisession, workers = 5)
```


```{r}
#Setting seed for reproducibility
set.seed(10)

#Splitting the data into training and test sets 
split<-initial_split(data, prop = 0.7)

#Printing the structure of the split object
split
```

```{r}
#Setting seed for reproducibility 
set.seed(5)

#Extracting training and test sets
train<-training(split)
test<-testing(split)

#Separating the training data into the 5 folds
folds<-vfold_cv(train, v = 5)

#Printing the structure of the folds object
folds
```


# Specifying Data Processing Steps

```{r}
#Constructing tidymodels recipe
recipe<-recipe(diabetes_binary ~ ., data = train)

#Printing recipe variable list with roles
recipe |>
  prep(training = train) |>
  summary()
```

# Classification Tree



```{r}
#Specifying model and engine for classification tree
tree_model<-decision_tree(tree_depth = tune(),
                          min_n = 5,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

#Creating classification tree workflow
tree_wkf<-workflow() |>
  add_recipe(recipe) |>
  add_model(tree_model)
```

```{r}
#Fitting classification tree model across grid of depth and complexity values
tree_fit<-tree_wkf |>
  tune_grid(resamples = folds,
            grid = grid_regular(tree_depth(), cost_complexity(), levels = c(10,10)),
            metrics = metric_set(mn_log_loss, accuracy)) 
```

```{r}
#Sorting models by log loss
tree_fit |> collect_metrics() |> filter(.metric=="mn_log_loss") |>
  arrange(mean)

#Sorting models by accuracy
tree_fit |> collect_metrics() |> filter(.metric=="accuracy") |>
  arrange(desc(mean))

```

```{r}
#Capturing best tree model
tree_best_params<-select_best(tree_fit, metric = "mn_log_loss")

#Printing hyperparameter values
tree_best_params
```



# Random Forest


```{r}
#Specifying model and engine for random forest
rf_model<-rand_forest(mtry = tune(),
                          min_n = 20,
                          trees = 500) |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("classification")

#Creating random forest workflow
rf_wkf<-workflow() |>
  add_recipe(recipe) |>
  add_model(rf_model)
```

```{r}
#Setting seed for reproducibility when rerunning chunk
set.seed(10)

#Fitting random forest model across values of mtry
rf_fit<-rf_wkf |>
  tune_grid(resamples = folds,
            grid = grid_regular(mtry(range = c(1,11)), levels = 11),
            metrics = metric_set(mn_log_loss, accuracy)) 
```

```{r}
#Sorting models by log loss
rf_fit |> collect_metrics() |> filter(.metric=="mn_log_loss") |>
  arrange(mean)

#Sorting models by accuracy
rf_fit |> collect_metrics() |> filter(.metric=="accuracy") |>
  arrange(mean)
```

```{r}
#Capturing best random forest model
rf_best_params<-select_best(rf_fit, metric = "mn_log_loss")

#Printing hyperparameter values
rf_best_params
```


#Comparing Performance on Test Set

```{r}
#Setting classification tree hyperparameter values to those selected via CV
tree_final_wkf<-tree_wkf |>
  finalize_workflow(tree_best_params)

#Fitting final model to the full training set and testing on original test set
tree_final_fit<-tree_final_wkf |>
  last_fit(split, metrics = metric_set(mn_log_loss, accuracy))
```

```{r}
#Setting seed for reproducibility when we rerun this chunk
set.seed(10)

#Setting rf hyperparameter values to those selected via CV
rf_final_wkf<-rf_wkf |>
  finalize_workflow(rf_best_params)

#Fitting final model to the full training set and testing on original test set
rf_final_fit<-rf_final_wkf |>
  last_fit(split, metrics = metric_set(mn_log_loss, accuracy))
```

```{r}
#Printing log loss and accuracy for the best models of each class
rbind( tree_final_fit |> collect_metrics() |> mutate(model = "Class Tree") |> select(model, everything()),
      rf_final_fit |> collect_metrics() |> mutate(model = "Random Forest") |> select(model, everything()))
```
While the two models produce the same test accuracy, the random forest model produces a lower log loss, indicating it is a superior model by this more sophisticated performance metric. Therefore, we will select this tuned random forest model as our final model and fit it to the entire dataset before taking it to production. 

```{r}
rf_production_fit<-rf_final_wkf |>
  fit(data)
```

Now that we have fit the final model, we can use it to make predictions. For example, the code below predicts whether or not an individual with the following characteristics has diabetes: 
- Does not exercise at least once per week
- Has high blood pressure
- Has high cholesterol
- Does not eat their fruits and vegetables
- Has a history of smoking
- Is not a heavy drinker
- Is male
- Is between the ages of 60 and 64
- Earns between \$15,000 and \$20,000 per year
- Has a BMI of 40

```{r}
new_obs<-tibble(.rows = 1) |>
  mutate(phys_activity = factor(0, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(high_bp = factor(1, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(high_chol = factor(1, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(fruits = factor(0, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(veggies = factor(0, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(smoker = factor(1, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(hvy_alc_consump = factor(0, levels = 0:1, labels = c("No", "Yes"))) |>
  mutate(sex = factor(1, levels = 0:1, labels = c("Female", "Male"))) |>
  mutate(age = factor(9, levels = 1:13, labels = c("18 to 24", "25 to 29",
                                                   "30 to 34", "35 to 39",
                                                   "40 to 44", "45 to 49",
                                                   "50 to 54", "55 to 59",
                                                   "60 to 64", "65 to 69",
                                                   "70 to 74", "75 to 79",
                                                   "80+"))) |>
  mutate(income = factor(3, levels = 1:8, labels = c("Less than $10k", "$10k to $15k",
                                                   "$15k to $20k", "$20k to $25k",
                                                   "$25k to $35k", "$35k to $50k",
                                                   "$50k to $75k", "$75k+"))) |>
  mutate(bmi = 40)

predict(rf_production_fit, new_data = new_obs)$.pred_class

#summary(predict(rf_production_fit, new_data = data))
```



