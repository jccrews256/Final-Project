[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Introduction\nDiabetes is a challenging disease for those who suffer from it, and it is also difficult to diagnose. However, predictive machine learning models may be able to effectively predict when someone has diabetes or is at least at greater risk. The goal of this project is to use easily collected characteristics on individuals to tune a predictive model of diabetes incidence.\nTo develop this model, we will utilize the 2015 survey responses from the Behavioral Risk Factor Surveillance System (BRFSS), an annual telephone-based survey attempting to characterize the state of American health. While over 400,000 individuals were surveyed via telephone, we analyze a cleaned version of the dataset that contains 253,680 responses. This dataset is available via Kaggle\nOur analysis will focus on the following variables, with cleaned and original variable names indicated: - diabetes_binary: Our response variable, a binary indicator for diabetes (original name: Diabetes_binary) - high_bp: A binary indicator for high blood pressure (original name: HighBP) - high_chol: A binary indicator for high cholesterol (original name: HighChol) - phys_activity: A binary indicator for whether the individual has exercised outside of work in the past 30 days (original name: PhysActivity) - fruits: A binary indicator for whether the individual consumes fruit at least once per day (original name: Fruits) - veggies: A binary indicator for whether the individual consumes vegetables at least once per day (original name: Veggies) - smoker: A binary indicator for whether the individual has smoked at least 100 cigarettes in their lifetime (original name: Smoker) - hvy_alc_consump: A binary indicator for whether the individual is considered a heavy drinker – 14 or more drinks per week for men and seven or more drinks per week for women (original name: HvyAlcoholConsump) - sex: A binary sex indicator – 0 for female and 1 for male (original name: Sex) - age: An age group indicator with 13 levels ranging from 18-24 to 80+ (original name: Age) - income: A household income income bracket indicator with 8 levels ranging from below $10,000 per year to over $75,000 per year (original name: Income)\nThis page documents our initial exploratory data analysis (EDA), including data validation steps and basic data summarization. Our summaries will be both univariate summary statistics and multi-variate summaries at each level of our response.\n\n\nLoading Packages and Reading in Data\nBefore beginning the EDA, we need to load the required packages and read in the data.\n\n#Loading packages\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(grid)\nlibrary(gridExtra)\n\nWarning: package 'gridExtra' was built under R version 4.3.3\n\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nWhen reading in the data, we will go ahead and subset the variables to only those we are interested in. Note that we have not yet cleaned the variable names. We will do so at the end of the next section.\n\n#Initially reading in data and subsetting to variables of interest\nraw_data&lt;-read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\") |&gt;\n  select(Diabetes_binary, HighBP, HighChol, PhysActivity, Fruits, Veggies, Smoker, HvyAlcoholConsump, Sex, Age, BMI, Income)\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nData Cleaning\nWe need to complete a few data validation steps to determine whether the data need to be cleaned before any summaries and analysis. To start, we will count the number of missing values for each variable.\n\n#Counting missing values\nraw_data |&gt;\n  summarize(across(everything(),~sum(is.na(.x))))\n\n# A tibble: 1 × 12\n  Diabetes_binary HighBP HighChol PhysActivity Fruits Veggies Smoker\n            &lt;int&gt;  &lt;int&gt;    &lt;int&gt;        &lt;int&gt;  &lt;int&gt;   &lt;int&gt;  &lt;int&gt;\n1               0      0        0            0      0       0      0\n# ℹ 5 more variables: HvyAlcoholConsump &lt;int&gt;, Sex &lt;int&gt;, Age &lt;int&gt;, BMI &lt;int&gt;,\n#   Income &lt;int&gt;\n\n\nFortunately, there are no missing values, so we can proceed to confirming each variable’s data type is appropriate.\n\n#Printing data structure\nstr(raw_data)\n\ntibble [253,680 × 12] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary  : num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP           : num [1:253680] 1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol         : num [1:253680] 1 0 1 0 1 1 0 1 1 0 ...\n $ PhysActivity     : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits           : num [1:253680] 0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies          : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ Smoker           : num [1:253680] 1 1 0 0 0 1 1 1 1 0 ...\n $ HvyAlcoholConsump: num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ Sex              : num [1:253680] 0 0 0 0 0 1 0 0 0 1 ...\n $ Age              : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ BMI              : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Income           : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n\n\nAll of these variables except BMI should actually be factors, but they are stored as numeric variables. Let’s convert them to factors and give them informative labels. However, let’s first confirm the levels of these factors are what we expect.\n\n#Printing unique values for the factor variables\napply(raw_data |&gt; select(!BMI), MARGIN = 2, unique)\n\n$Diabetes_binary\n[1] 0 1\n\n$HighBP\n[1] 1 0\n\n$HighChol\n[1] 1 0\n\n$PhysActivity\n[1] 0 1\n\n$Fruits\n[1] 0 1\n\n$Veggies\n[1] 1 0\n\n$Smoker\n[1] 1 0\n\n$HvyAlcoholConsump\n[1] 0 1\n\n$Sex\n[1] 0 1\n\n$Age\n [1]  9  7 11 10  8 13  4  6  2 12  5  1  3\n\n$Income\n[1] 3 1 8 6 4 7 2 5\n\n\nThese values are exactly as we expect: the first nine variables are binary. The Age variable is a multi-level factor ranging from a value of 1 for individuals between 18 and 24 to a value of 13 for those at least 80 years old. The Income variable is also a multi-level factor ranging from a value of 1 for individuals with household incomes below $10,000 to a value of 8 for individuals with household incomes above $75,000.\nNow we need to ensure the BMI variable takes on reasonable values, which we can do by extracting the minimum and maximum within our sample.\n\n#Capturing min and max of BMI\nraw_data |&gt;\n  summarize(across(BMI, list(min = min, max = max), .names = \"{.col}_{.fn}\"))\n\n# A tibble: 1 × 2\n  BMI_min BMI_max\n    &lt;dbl&gt;   &lt;dbl&gt;\n1      12      98\n\n\nWhile a BMI of 98 is very high, it is still plausible. Thus, we can reasonably assume the BMI values are correct.\nThankfully, these data are very clean. We are now ready to convert all the variables except BMI to factors and assign appropriate labels. In doing so, let’s change all the variable names to the clean ones listed in the introduction.\n\n#Converting most variables to factors (with labels) and updating names\ndata&lt;-raw_data |&gt;\n  #Converting yes-no variables\n  mutate(diabetes_binary = factor(Diabetes_binary, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(high_bp = factor(HighBP, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(high_chol = factor(HighChol, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(phys_activity = factor(PhysActivity, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(fruits = factor(Fruits, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(veggies = factor(Veggies, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(smoker = factor(Smoker, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(hvy_alc_consump = factor(HvyAlcoholConsump, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  #Converting sex to factor\n  mutate(sex = factor(Sex, levels = 0:1, labels = c(\"Female\", \"Male\"))) |&gt;\n  #Converting age to factor\n  mutate(age = factor(Age, levels = 1:13, labels = c(\"18 to 24\", \"25 to 29\",\n                                                     \"30 to 34\", \"35 to 39\",\n                                                     \"40 to 44\", \"45 to 49\",\n                                                     \"50 to 54\", \"55 to 59\",\n                                                     \"60 to 64\", \"65 to 69\",\n                                                     \"70 to 74\", \"75 to 79\",\n                                                     \"80+\"))) |&gt; \n  #Converting income to factor\n  mutate(income = factor(Income, levels = 1:8, labels = c(\"Less than $10k\", \"$10k to $15k\",\n                                                     \"$15k to $20k\", \"$20k to $25k\",\n                                                     \"$25k to $35k\", \"$35k to $50k\",\n                                                     \"$50k to $75k\", \"$75k+\"))) |&gt; \n  #Renaming bmi\n  rename(bmi = BMI) |&gt;\n  #Dropping original \"factor\" variables\n  select(!c(Diabetes_binary:Age, Income))\n\n#Printing structure of resultant data\nstr(data)\n\ntibble [253,680 × 12] (S3: tbl_df/tbl/data.frame)\n $ bmi            : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ diabetes_binary: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ high_bp        : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 1 ...\n $ high_chol      : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ phys_activity  : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ fruits         : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ veggies        : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ smoker         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 1 2 2 2 2 1 ...\n $ hvy_alc_consump: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ sex            : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ age            : Factor w/ 13 levels \"18 to 24\",\"25 to 29\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ income         : Factor w/ 8 levels \"Less than $10k\",..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\nNote that when we print the data structure this time, each variable except bmi is a factor with interpretable labels.\n\n\nSummaries\nWe are now ready to summarize our data. To begin, we will produce univariate numeric summaries. We will then produce visual summaries of the multivariate distributions of our response and each predictor.\n\nUnivariate Summaries\nFor our binary categorical variables other than sex, let’s produce a single table to compare the breakdown across yes and no.\n\n#Generating basic counts for yes-no variables\ndata |&gt;\n  #Subsetting to the variables\n  select(!c(age, sex, bmi, income)) |&gt;\n  #Pivoting long for easier tabulations\n  pivot_longer(cols = everything(), names_to = \"variables\", values_to = \"values\") |&gt;\n  #Grouping by variables and their levels\n  group_by(variables, values) |&gt;\n  #Counting by group\n  summarize(count = n()) |&gt;\n  #Pivoting wide by values for cleanliness\n  pivot_wider(names_from = values, values_from = count)\n\n`summarise()` has grouped output by 'variables'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   variables [8]\n  variables           No    Yes\n  &lt;chr&gt;            &lt;int&gt;  &lt;int&gt;\n1 diabetes_binary 218334  35346\n2 fruits           92782 160898\n3 high_bp         144851 108829\n4 high_chol       146089 107591\n5 hvy_alc_consump 239424  14256\n6 phys_activity    61760 191920\n7 smoker          141257 112423\n8 veggies          47839 205841\n\n\nLooking at these univariate distributions, one key finding is that we have substantial class imbalance in our response variable. This is not particularly concerning given the large sample size, but it is something to be cognizant of if we prefer to over-predict rather than under-predict diabetes with our screener model.\nBefore we move to multi-variate distributions, let’s look at the univariate breakdowns for sex, age, and income, as well as some numeric summaries for bmi. We will start with sex.\n\n#Generating observation counts by sex\ndata |&gt;\n  group_by(sex) |&gt;\n  summarize(count = n()) \n\n# A tibble: 2 × 2\n  sex     count\n  &lt;fct&gt;   &lt;int&gt;\n1 Female 141974\n2 Male   111706\n\n\nIt is notable that 56% of the observations correspond to females, compared to roughly 51% of the national population. It may have been the case that more of the male responses were culled when the person who created this dataset was cleaning the original response file.\nLet’s move onto age.\n\n#Generating counts by age group\ndata |&gt;\n  group_by(age) |&gt;\n  summarize(count = n()) |&gt;\n  print(n = 13)\n\n# A tibble: 13 × 2\n   age      count\n   &lt;fct&gt;    &lt;int&gt;\n 1 18 to 24  5700\n 2 25 to 29  7598\n 3 30 to 34 11123\n 4 35 to 39 13823\n 5 40 to 44 16157\n 6 45 to 49 19819\n 7 50 to 54 26314\n 8 55 to 59 30832\n 9 60 to 64 33244\n10 65 to 69 32194\n11 70 to 74 23533\n12 75 to 79 15980\n13 80+      17363\n\n\nGiven that the survey is meant to be representative of the broader US population, the general upward trend in responses with age is a bit concerning. Could this be because younger generations are harder to contact via phone?\nOnto income!\n\n#Generating counts by income bracket\ndata |&gt;\n  group_by(income) |&gt;\n  summarize(count = n())\n\n# A tibble: 8 × 2\n  income         count\n  &lt;fct&gt;          &lt;int&gt;\n1 Less than $10k  9811\n2 $10k to $15k   11783\n3 $15k to $20k   15994\n4 $20k to $25k   20135\n5 $25k to $35k   25883\n6 $35k to $50k   36470\n7 $50k to $75k   43219\n8 $75k+          90385\n\n\nFor household income, we see that the most common income group is $75,000+. Thus, these income groupings may not be particularly helpful for segmenting the population our sample represents, as we may be missing nuanced differences across income intervals above $75,000.\nWhile we have already captured the minimum and maximum BMI values, we should explore a few other summary statistics for our lone numeric variable.\n\n#Generating summary stats for bmi\ndata |&gt;\n  summarize(across(bmi, list(mean = mean, median =median, sd = sd, IQR = IQR)))\n\n# A tibble: 1 × 4\n  bmi_mean bmi_median bmi_sd bmi_IQR\n     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1     28.4         27   6.61       7\n\n\nPerhaps surprisingly, given that BMI is naturally bounded at the low end, the BMI values in our sample are not excessively skewed; the mean and median are relatively close together.\n\n\nMultivariate Summaries\nTo start our multivariate analysis, let’s plot our response by each level of our yes-no variables. Given that we have already explored the unconditional distributions for these variables, we will use filled bar plots to focus specifically on the differences in diabetes rates across these conditional distributions.\n\n#Generating response breakdown by phys_activity\ng1&lt;-ggplot(data = data, aes(x = phys_activity, fill = diabetes_binary)) + geom_bar(position = \"fill\") + labs(x = \"Physical Activity?\", y = \"Share of Obs.\", fill = \"Diabetes?\")\n\n#Generating response breakdown by high_bp\ng2&lt;-ggplot(data = data, aes(x = high_bp, fill = diabetes_binary)) + geom_bar(position = \"fill\") + labs(x = \"High Blood Pressure?\", y = \"Share of Obs.\", fill = \"Diabetes?\")\n\n#Generating response breakdown by high_chol\ng3&lt;-ggplot(data = data, aes(x = high_chol, fill = diabetes_binary)) + geom_bar(position = \"fill\") + labs(x = \"High Cholesterol?\", y = \"Share of Obs.\", fill = \"Diabetes?\")\n\n#Generating response breakdown by fruits\ng4&lt;-ggplot(data = data, aes(x = fruits, fill = diabetes_binary)) + geom_bar(position = \"fill\") + labs(x = \"Eat Fruit?\", y = \"Share of Obs.\", fill = \"Diabetes?\")\n\n#Generating response breakdown by veggies\ng5&lt;-ggplot(data = data, aes(x = veggies, fill = diabetes_binary)) + geom_bar(position = \"fill\") + labs(x = \"Eat Veggies?\", y = \"Share of Obs.\", fill = \"Diabetes?\")\n\n#Generating response breakdown by smoker\ng6&lt;-ggplot(data = data, aes(x = smoker, fill = diabetes_binary)) + geom_bar(position = \"fill\") + labs(x = \"Smoking History?\", y = \"Share of Obs.\", fill = \"Diabetes?\")\n\n#Generating response breakdown by hvy_alc_consump\ng7&lt;-ggplot(data = data, aes(x = hvy_alc_consump, fill = diabetes_binary)) + geom_bar(position = \"fill\") + labs(x = \"Heavy Alcohol Consumption?\", y = \"Share of Obs.\", fill = \"Diabetes?\")\n\n#Combining the seven plots in a single visualations\ngrid.arrange(g1, g2, g3, g4, g5, g6, g7, ncol = 2, top = textGrob(\"Diabetes Frequency across Levels of Behavioral Variables\", hjust = 1.15, gp = gpar(fontsize = 18)))\n\n\n\n\n\n\n\n\nIt seems that, unconditional on other variables, physical activity and heavy alcohol consumption are the behaviors most closely related to changes in diabetes frequency. Perhaps surprisingly, the diabetes rate is lower among individuals with relatively high alcohol consumption.\nDiabetes rates are also much higher among those with high cholesterol and high blood pressure. These must be common co-morbidities for diabetes.\nLet’s now explore any differences in diabetes frequency across sex to see whether this demographic characteristic explains diabetes incidence on its own.\n\n#Generating response breakdown by sex\nggplot(data = data, aes(x = sex, fill = diabetes_binary)) + geom_bar(position = \"fill\") + labs(x = \"Sex\", y = \"Share of Obs.\", fill = \"Diabetes?\", title = \"Diabetes Frequency by Sex\")\n\n\n\n\n\n\n\n\nIt seems males in this dataset are slightly more likely to have diabetes.\nNow let’s compare diabetes frequencies across age groups.\n\n#Generating diabetes breakdown by age group\nggplot(data = data, aes(x = age, fill = diabetes_binary)) + geom_bar(position = \"fill\") + labs(x = \"Age Group\", y = \"Share of Observations\", fill = \"Diabetes?\", title = \"Diabetes Frequency by Age Group\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\n\n\n\n\n\n\nAt least in our sample, diabetes rates increase with age until we reach the oldest age groups; the decline for older ages may reflect reduced life expectancy due to diabetes.\nCould household income be a predictor of diabetes? If so, there could be important implications for health interventions.\n\n#Generating diabetes breakdown by income bracket\nggplot(data = data, aes(x = income, fill = diabetes_binary)) + geom_bar(position = \"fill\") + labs(x = \"Income Bracket\", y = \"Share of Observations\", fill = \"Diabetes?\", title = \"Diabetes Frequency by Household Income Bracket\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\n\n\n\n\n\n\nIt seems diabetes risk declines as household income rises. This may indicate that access to less processed foods, which can be more expensive, reduces the risk of diabetes. However, it could also be the case the higher incomes correspond with increased access to early detection tools and preventative treatments.\nFor our final visuals summary, let’s produce BMI box plots for those with and without diabetes to see if the BMI distributions vary.\n\n#bmi distributions by diabetes_binary\nggplot(data = data, aes(x = diabetes_binary, y = bmi, fill = diabetes_binary)) + geom_boxplot() + \n  labs(title = \"BMI Distribution by Diabetes Diagnosis\", y = \"BMI\", x = \"Diabetes?\") + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIt does seem that those with diabetes have higher BMIs on average, but these distributions almost completely overlap.\n\n\n\nWrap-Up\nThis concludes our exploratory data analysis. We will now attempt to predict whether or not an individual has diabetes using only the variables explored on this page."
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "A Predictive Model for Diabetes",
    "section": "",
    "text": "Introduction\nDiabetes is a challenging disease for those who suffer from it, and it is also difficult to diagnose. However, predictive machine learning models may be able to effectively predict when someone has diabetes or is at least at greater risk. The goal of this project is to use easily collected characteristics on individuals to tune a predictive model of diabetes incidence.\nTo develop this model, we will utilize the 2015 survey responses from the Behavioral Risk Factor Surveillance System (BRFSS), an annual telephone-based survey attempting to characterize the state of American health. While over 400,000 individuals were surveyed via telephone, we analyze a cleaned version of the dataset that contains 253,680 responses. This dataset is made available by Alex Teboul via Kaggle.\nOur analysis will focus on the following variables, with cleaned and original variable names indicated: - diabetes_binary: Our response variable, a binary indicator for diabetes (original name: Diabetes_binary) - high_bp: A binary indicator for high blood pressure (original name: HighBP) - high_chol: A binary indicator for high cholesterol (original name: HighChol) - phys_activity: A binary indicator for whether the individual has exercised outside of work in the past 30 days (original name: PhysActivity) - fruits: A binary indicator for whether the individual consumes fruit at least once per day (original name: Fruits) - veggies: A binary indicator for whether the individual consumes vegetables at least once per day (original name: Veggies) - smoker: A binary indicator for whether the individual has smoked at least 100 cigarettes in their lifetime (original name: Smoker) - hvy_alc_consump: A binary indicator for whether the individual is considered a heavy drinker – 14 or more drinks per week for men and seven or more drinks per week for women (original name: HvyAlcoholConsump) - sex: A binary sex indicator – 0 for female and 1 for male (original name: Sex) - age: An age group indicator with 13 levels ranging from 18-24 to 80+ (original name: Age) - income: A household income income bracket indicator with 8 levels ranging from below $10,000 per year to over $75,000 per year (original name: Income)\nThis page documents our efforts to tune two model classes in producing a final predictive model of diabetes incidence. Those two model classes are the classification tree and the random forest, which will each be described in detail below. To tune the models and select a final model, we take the following approach: 1. Split the data into a training set containing 70% of the data and a test set containing 30% of the data. 2. Tune each model class using the training set via 5-fold cross-validation, using log loss to select the best model from each class. 3. Fit each “best” model to the full training set and predict for the full test set, selecting the final model with the best log loss. 4. Fit the overall best model to the full dataset so that it can be put into production.\n\n\nLoading Packages and Processing the Data\nBefore we begin modeling, let’s load in the necessary packages and also read in and process the data. Given the size of the dataset, let’s utilize five cores to speed up processing time.\n\n#Reading in packages\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tune         2.0.1 \n✔ infer        1.0.9      ✔ workflows    1.3.0 \n✔ modeldata    1.5.1      ✔ workflowsets 1.1.1 \n✔ parsnip      1.3.3      ✔ yardstick    1.3.2 \n✔ recipes      1.3.1      \n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(rsample)\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.3.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.3.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(future)\n\nWarning: package 'future' was built under R version 4.3.3\n\nlibrary(knitr)\n\n#Utilizing parallel processing with 5 cores\nplan(multisession, workers = 5)\n\nThe code below reads in the data and applies the same processing steps used on the exploratory data analysis page.\n\n#Reading in data and subsetting to variables of interest\nraw_data&lt;-read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\") |&gt;\n  select(Diabetes_binary, HighBP, HighChol, PhysActivity, Fruits, Veggies, Smoker, HvyAlcoholConsump, Sex, Age, BMI, Income)\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Converting factor variables to true factors with appropriate labels, renaming with clean names\ndata&lt;-raw_data |&gt;\n  #Converting yes-no variables to factors\n  mutate(diabetes_binary = factor(Diabetes_binary, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(high_bp = factor(HighBP, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(high_chol = factor(HighChol, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(phys_activity = factor(PhysActivity, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(fruits = factor(Fruits, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(veggies = factor(Veggies, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(smoker = factor(Smoker, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(hvy_alc_consump = factor(HvyAlcoholConsump, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  #Converting sex to factor\n  mutate(sex = factor(Sex, levels = 0:1, labels = c(\"Female\", \"Male\"))) |&gt;\n  #Converting age to factor\n  mutate(age = factor(Age, levels = 1:13, labels = c(\"18 to 24\", \"25 to 29\",\n                                                     \"30 to 34\", \"35 to 39\",\n                                                     \"40 to 44\", \"45 to 49\",\n                                                     \"50 to 54\", \"55 to 59\",\n                                                     \"60 to 64\", \"65 to 69\",\n                                                     \"70 to 74\", \"75 to 79\",\n                                                     \"80+\"))) |&gt; \n  #Converting income to factor\n  mutate(income = factor(Income, levels = 1:8, labels = c(\"Less than $10k\", \"$10k to $15k\",\n                                                     \"$15k to $20k\", \"$20k to $25k\",\n                                                     \"$25k to $35k\", \"$35k to $50k\",\n                                                     \"$50k to $75k\", \"$75k+\"))) |&gt; \n  #Renaming bmi\n  rename(bmi = BMI) |&gt;\n  #Dropping original \"factors\" \n  select(!c(Diabetes_binary:Age, Income))\n\n\n\nPreparing the Data for Model Tuning\nBefore we begin fitting models, we need to split the data into training and test set and also create our five cross-validation folds. Let’s start by splitting the data.\n\n#Setting seed for reproducibility\nset.seed(10)\n\n#Splitting the data into training and test sets \nsplit&lt;-initial_split(data, prop = 0.7)\n\n#Printing the structure of the split object\nsplit\n\n&lt;Training/Testing/Total&gt;\n&lt;177576/76104/253680&gt;\n\n\nBy printing the split object, we can see that we have split the data into 70% and 30% chunks.\nNow we will create the five folds for tuning.\n\n#Setting seed for reproducibility \nset.seed(5)\n\n#Extracting training and test sets\ntrain&lt;-training(split)\ntest&lt;-testing(split)\n\n#Separating the training data into the 5 folds\nfolds&lt;-vfold_cv(train, v = 5)\n\n#Printing the structure of the folds object\nfolds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits                 id   \n  &lt;list&gt;                 &lt;chr&gt;\n1 &lt;split [142060/35516]&gt; Fold1\n2 &lt;split [142061/35515]&gt; Fold2\n3 &lt;split [142061/35515]&gt; Fold3\n4 &lt;split [142061/35515]&gt; Fold4\n5 &lt;split [142061/35515]&gt; Fold5\n\n\nPrinting the folds object reflects the division of our training set into five roughly equal parts.\n\n\nTuning the Classification Tree\nIn many ways, the classification tree model is the most conceptually simple model. We build a single tree that functions as a decision rule for predictions. Each split in the tree is successively chosen such that it maximally reduces “impurity” of response classes in the resultant branches. That is, we choose splits such that the observations at the end of each branch are maximally homogeneous. Once we have fit a tree, we make predictions via majority rule within each endpoint.\nTo tune a classification tree, we generally adjust two hyperparameters: 1. Tree depth, or the maximum number of splits leading to a single branch; a lower value will grow a “wider” tree 2. Cost complexity, which is a hyperparameter that controls the degree of pruning (removal of splits) to prevent overfitting; higher values correspond to more pruning\nBefore we tune our classification tree, let’s specify the recipe we will use. This is a simple recipe, as most tree-based models do not require us to convert factors to dummies or specific non-linear effects; they naturally handle such complexities. By prepping the recipe and printing a summary, we can confirm that we has assigned the correct roles to each variable. Note that this is the same recipe we will use for the random forest model.\n\n#Constructing tidymodels recipe\nrecipe&lt;-recipe(diabetes_binary ~ ., data = train)\n\n#Printing recipe variable list with roles\nrecipe |&gt;\n  prep(training = train) |&gt;\n  summary() |&gt;\n  print(n = 12)\n\n# A tibble: 12 × 4\n   variable        type      role      source  \n   &lt;chr&gt;           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 bmi             &lt;chr [2]&gt; predictor original\n 2 high_bp         &lt;chr [3]&gt; predictor original\n 3 high_chol       &lt;chr [3]&gt; predictor original\n 4 phys_activity   &lt;chr [3]&gt; predictor original\n 5 fruits          &lt;chr [3]&gt; predictor original\n 6 veggies         &lt;chr [3]&gt; predictor original\n 7 smoker          &lt;chr [3]&gt; predictor original\n 8 hvy_alc_consump &lt;chr [3]&gt; predictor original\n 9 sex             &lt;chr [3]&gt; predictor original\n10 age             &lt;chr [3]&gt; predictor original\n11 income          &lt;chr [3]&gt; predictor original\n12 diabetes_binary &lt;chr [3]&gt; outcome   original\n\n\nEverything looks good, so we can now create a workflow for our tuning process. We will indicate that we want to tune the tree depth and cost complexity while not allowing any tree endpoint to contain fewer than 20 observations.\n\n#Specifying model and engine for classification tree\ntree_model&lt;-decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\n#Creating classification tree workflow\ntree_wkf&lt;-workflow() |&gt;\n  add_recipe(recipe) |&gt;\n  add_model(tree_model)\n\nWe are now ready to tune. To keep computation costs relatively low, we will fit models for a grid of 10 tree depths and 10 cost complexity values (100 total models). Remember that we are refitting each individual model five times (5-fold CV), so we are actually fitting 500 total models.\n\n#Fitting classification tree model across grid of depth and complexity values\ntree_fit&lt;-tree_wkf |&gt;\n  tune_grid(resamples = folds,\n            grid = grid_regular(tree_depth(), cost_complexity(), levels = c(10,10)),\n            metrics = metric_set(mn_log_loss, accuracy)) \n\nNow that we have fit the models, we can print the average log loss values across the five folds to identify the best combination of tree depth and cost complexity. While we will selected the best model by minimizing the log loss value, we will also print average accuracy values so that we can obtain a better conceptual understanding of predictive capacity.\n\n#Sorting models by log loss\ntree_fit |&gt; collect_metrics() |&gt; filter(.metric==\"mn_log_loss\") |&gt;\n  arrange(mean) |&gt;\n  kable()\n\nWarning: 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\nWarning: 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncost_complexity\ntree_depth\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n0e+00\n10\nmn_log_loss\nbinary\n0.3440640\n5\n0.0022177\npre0_mod007_post0\n\n\n0e+00\n10\nmn_log_loss\nbinary\n0.3440640\n5\n0.0022177\npre0_mod017_post0\n\n\n0e+00\n10\nmn_log_loss\nbinary\n0.3440640\n5\n0.0022177\npre0_mod027_post0\n\n\n1e-07\n10\nmn_log_loss\nbinary\n0.3440640\n5\n0.0022177\npre0_mod037_post0\n\n\n1e-06\n10\nmn_log_loss\nbinary\n0.3440640\n5\n0.0022177\npre0_mod047_post0\n\n\n1e-05\n10\nmn_log_loss\nbinary\n0.3449960\n5\n0.0025892\npre0_mod057_post0\n\n\n0e+00\n11\nmn_log_loss\nbinary\n0.3455059\n5\n0.0020631\npre0_mod008_post0\n\n\n0e+00\n11\nmn_log_loss\nbinary\n0.3455059\n5\n0.0020631\npre0_mod018_post0\n\n\n0e+00\n11\nmn_log_loss\nbinary\n0.3455059\n5\n0.0020631\npre0_mod028_post0\n\n\n1e-07\n11\nmn_log_loss\nbinary\n0.3455059\n5\n0.0020631\npre0_mod038_post0\n\n\n1e-06\n11\nmn_log_loss\nbinary\n0.3455059\n5\n0.0020631\npre0_mod048_post0\n\n\n1e-05\n11\nmn_log_loss\nbinary\n0.3468333\n5\n0.0027492\npre0_mod058_post0\n\n\n0e+00\n13\nmn_log_loss\nbinary\n0.3499877\n5\n0.0018421\npre0_mod009_post0\n\n\n0e+00\n13\nmn_log_loss\nbinary\n0.3499877\n5\n0.0018421\npre0_mod019_post0\n\n\n0e+00\n13\nmn_log_loss\nbinary\n0.3499877\n5\n0.0018421\npre0_mod029_post0\n\n\n1e-07\n13\nmn_log_loss\nbinary\n0.3499877\n5\n0.0018421\npre0_mod039_post0\n\n\n1e-06\n13\nmn_log_loss\nbinary\n0.3499877\n5\n0.0018421\npre0_mod049_post0\n\n\n0e+00\n8\nmn_log_loss\nbinary\n0.3500823\n5\n0.0016814\npre0_mod006_post0\n\n\n0e+00\n8\nmn_log_loss\nbinary\n0.3500823\n5\n0.0016814\npre0_mod016_post0\n\n\n0e+00\n8\nmn_log_loss\nbinary\n0.3500823\n5\n0.0016814\npre0_mod026_post0\n\n\n1e-07\n8\nmn_log_loss\nbinary\n0.3500823\n5\n0.0016814\npre0_mod036_post0\n\n\n1e-06\n8\nmn_log_loss\nbinary\n0.3500823\n5\n0.0016814\npre0_mod046_post0\n\n\n1e-05\n8\nmn_log_loss\nbinary\n0.3500823\n5\n0.0016814\npre0_mod056_post0\n\n\n1e-05\n13\nmn_log_loss\nbinary\n0.3512939\n5\n0.0022718\npre0_mod059_post0\n\n\n0e+00\n7\nmn_log_loss\nbinary\n0.3529256\n5\n0.0010952\npre0_mod005_post0\n\n\n0e+00\n7\nmn_log_loss\nbinary\n0.3529256\n5\n0.0010952\npre0_mod015_post0\n\n\n0e+00\n7\nmn_log_loss\nbinary\n0.3529256\n5\n0.0010952\npre0_mod025_post0\n\n\n1e-07\n7\nmn_log_loss\nbinary\n0.3529256\n5\n0.0010952\npre0_mod035_post0\n\n\n1e-06\n7\nmn_log_loss\nbinary\n0.3529256\n5\n0.0010952\npre0_mod045_post0\n\n\n1e-05\n7\nmn_log_loss\nbinary\n0.3529256\n5\n0.0010952\npre0_mod055_post0\n\n\n1e-04\n8\nmn_log_loss\nbinary\n0.3533248\n5\n0.0005343\npre0_mod066_post0\n\n\n1e-04\n7\nmn_log_loss\nbinary\n0.3538047\n5\n0.0004467\npre0_mod065_post0\n\n\n1e-04\n10\nmn_log_loss\nbinary\n0.3539422\n5\n0.0005017\npre0_mod067_post0\n\n\n1e-04\n13\nmn_log_loss\nbinary\n0.3540918\n5\n0.0011098\npre0_mod069_post0\n\n\n1e-04\n11\nmn_log_loss\nbinary\n0.3546547\n5\n0.0009121\npre0_mod068_post0\n\n\n1e-04\n15\nmn_log_loss\nbinary\n0.3549963\n5\n0.0011518\npre0_mod070_post0\n\n\n1e-03\n8\nmn_log_loss\nbinary\n0.3573408\n5\n0.0012439\npre0_mod076_post0\n\n\n1e-03\n10\nmn_log_loss\nbinary\n0.3573535\n5\n0.0012496\npre0_mod077_post0\n\n\n1e-03\n11\nmn_log_loss\nbinary\n0.3573535\n5\n0.0012496\npre0_mod078_post0\n\n\n1e-03\n13\nmn_log_loss\nbinary\n0.3573535\n5\n0.0012496\npre0_mod079_post0\n\n\n1e-03\n15\nmn_log_loss\nbinary\n0.3573535\n5\n0.0012496\npre0_mod080_post0\n\n\n1e-03\n7\nmn_log_loss\nbinary\n0.3573899\n5\n0.0012654\npre0_mod075_post0\n\n\n0e+00\n5\nmn_log_loss\nbinary\n0.3575100\n5\n0.0012512\npre0_mod004_post0\n\n\n0e+00\n5\nmn_log_loss\nbinary\n0.3575100\n5\n0.0012512\npre0_mod014_post0\n\n\n0e+00\n5\nmn_log_loss\nbinary\n0.3575100\n5\n0.0012512\npre0_mod024_post0\n\n\n1e-07\n5\nmn_log_loss\nbinary\n0.3575100\n5\n0.0012512\npre0_mod034_post0\n\n\n1e-06\n5\nmn_log_loss\nbinary\n0.3575100\n5\n0.0012512\npre0_mod044_post0\n\n\n1e-05\n5\nmn_log_loss\nbinary\n0.3575100\n5\n0.0012512\npre0_mod054_post0\n\n\n1e-04\n5\nmn_log_loss\nbinary\n0.3575100\n5\n0.0012512\npre0_mod064_post0\n\n\n1e-03\n5\nmn_log_loss\nbinary\n0.3575100\n5\n0.0012512\npre0_mod074_post0\n\n\n0e+00\n15\nmn_log_loss\nbinary\n0.3610254\n5\n0.0017936\npre0_mod010_post0\n\n\n0e+00\n15\nmn_log_loss\nbinary\n0.3610254\n5\n0.0017936\npre0_mod020_post0\n\n\n0e+00\n15\nmn_log_loss\nbinary\n0.3610254\n5\n0.0017936\npre0_mod030_post0\n\n\n1e-07\n15\nmn_log_loss\nbinary\n0.3610254\n5\n0.0017936\npre0_mod040_post0\n\n\n1e-06\n15\nmn_log_loss\nbinary\n0.3610254\n5\n0.0017936\npre0_mod050_post0\n\n\n1e-05\n15\nmn_log_loss\nbinary\n0.3613752\n5\n0.0018487\npre0_mod060_post0\n\n\n0e+00\n4\nmn_log_loss\nbinary\n0.3757583\n5\n0.0110723\npre0_mod003_post0\n\n\n0e+00\n4\nmn_log_loss\nbinary\n0.3757583\n5\n0.0110723\npre0_mod013_post0\n\n\n0e+00\n4\nmn_log_loss\nbinary\n0.3757583\n5\n0.0110723\npre0_mod023_post0\n\n\n1e-07\n4\nmn_log_loss\nbinary\n0.3757583\n5\n0.0110723\npre0_mod033_post0\n\n\n1e-06\n4\nmn_log_loss\nbinary\n0.3757583\n5\n0.0110723\npre0_mod043_post0\n\n\n1e-05\n4\nmn_log_loss\nbinary\n0.3757583\n5\n0.0110723\npre0_mod053_post0\n\n\n1e-04\n4\nmn_log_loss\nbinary\n0.3757583\n5\n0.0110723\npre0_mod063_post0\n\n\n0e+00\n1\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod001_post0\n\n\n0e+00\n2\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod002_post0\n\n\n0e+00\n1\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod011_post0\n\n\n0e+00\n2\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod012_post0\n\n\n0e+00\n1\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod021_post0\n\n\n0e+00\n2\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod022_post0\n\n\n1e-07\n1\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod031_post0\n\n\n1e-07\n2\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod032_post0\n\n\n1e-06\n1\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod041_post0\n\n\n1e-06\n2\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod042_post0\n\n\n1e-05\n1\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod051_post0\n\n\n1e-05\n2\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod052_post0\n\n\n1e-04\n1\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod061_post0\n\n\n1e-04\n2\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod062_post0\n\n\n1e-03\n1\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod071_post0\n\n\n1e-03\n2\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod072_post0\n\n\n1e-03\n4\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod073_post0\n\n\n1e-02\n1\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod081_post0\n\n\n1e-02\n2\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod082_post0\n\n\n1e-02\n4\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod083_post0\n\n\n1e-02\n5\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod084_post0\n\n\n1e-02\n7\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod085_post0\n\n\n1e-02\n8\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod086_post0\n\n\n1e-02\n10\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod087_post0\n\n\n1e-02\n11\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod088_post0\n\n\n1e-02\n13\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod089_post0\n\n\n1e-02\n15\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod090_post0\n\n\n1e-01\n1\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod091_post0\n\n\n1e-01\n2\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod092_post0\n\n\n1e-01\n4\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod093_post0\n\n\n1e-01\n5\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod094_post0\n\n\n1e-01\n7\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod095_post0\n\n\n1e-01\n8\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod096_post0\n\n\n1e-01\n10\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod097_post0\n\n\n1e-01\n11\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod098_post0\n\n\n1e-01\n13\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod099_post0\n\n\n1e-01\n15\nmn_log_loss\nbinary\n0.4036799\n5\n0.0010036\npre0_mod100_post0\n\n\n\n\n#Sorting models by accuracy\ntree_fit |&gt; collect_metrics() |&gt; filter(.metric==\"accuracy\") |&gt;\n  arrange(desc(mean)) |&gt;\n  kable()\n\nWarning: 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\nWarning: 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncost_complexity\ntree_depth\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n1e-03\n8\naccuracy\nbinary\n0.8629319\n5\n0.0008648\npre0_mod076_post0\n\n\n1e-03\n10\naccuracy\nbinary\n0.8628756\n5\n0.0008983\npre0_mod077_post0\n\n\n1e-03\n11\naccuracy\nbinary\n0.8628756\n5\n0.0008983\npre0_mod078_post0\n\n\n1e-03\n13\naccuracy\nbinary\n0.8628756\n5\n0.0008983\npre0_mod079_post0\n\n\n1e-03\n15\naccuracy\nbinary\n0.8628756\n5\n0.0008983\npre0_mod080_post0\n\n\n1e-03\n7\naccuracy\nbinary\n0.8627574\n5\n0.0007890\npre0_mod075_post0\n\n\n1e-04\n8\naccuracy\nbinary\n0.8627405\n5\n0.0008214\npre0_mod066_post0\n\n\n0e+00\n7\naccuracy\nbinary\n0.8626785\n5\n0.0008105\npre0_mod005_post0\n\n\n0e+00\n7\naccuracy\nbinary\n0.8626785\n5\n0.0008105\npre0_mod015_post0\n\n\n0e+00\n7\naccuracy\nbinary\n0.8626785\n5\n0.0008105\npre0_mod025_post0\n\n\n1e-07\n7\naccuracy\nbinary\n0.8626785\n5\n0.0008105\npre0_mod035_post0\n\n\n1e-06\n7\naccuracy\nbinary\n0.8626785\n5\n0.0008105\npre0_mod045_post0\n\n\n1e-05\n7\naccuracy\nbinary\n0.8626785\n5\n0.0008105\npre0_mod055_post0\n\n\n1e-04\n7\naccuracy\nbinary\n0.8626504\n5\n0.0008038\npre0_mod065_post0\n\n\n0e+00\n8\naccuracy\nbinary\n0.8626166\n5\n0.0007280\npre0_mod006_post0\n\n\n0e+00\n8\naccuracy\nbinary\n0.8626166\n5\n0.0007280\npre0_mod016_post0\n\n\n0e+00\n8\naccuracy\nbinary\n0.8626166\n5\n0.0007280\npre0_mod026_post0\n\n\n1e-07\n8\naccuracy\nbinary\n0.8626166\n5\n0.0007280\npre0_mod036_post0\n\n\n1e-06\n8\naccuracy\nbinary\n0.8626166\n5\n0.0007280\npre0_mod046_post0\n\n\n1e-05\n8\naccuracy\nbinary\n0.8626166\n5\n0.0007280\npre0_mod056_post0\n\n\n1e-04\n10\naccuracy\nbinary\n0.8624871\n5\n0.0008741\npre0_mod067_post0\n\n\n1e-04\n11\naccuracy\nbinary\n0.8624476\n5\n0.0007048\npre0_mod068_post0\n\n\n0e+00\n5\naccuracy\nbinary\n0.8622505\n5\n0.0006767\npre0_mod004_post0\n\n\n0e+00\n5\naccuracy\nbinary\n0.8622505\n5\n0.0006767\npre0_mod014_post0\n\n\n0e+00\n5\naccuracy\nbinary\n0.8622505\n5\n0.0006767\npre0_mod024_post0\n\n\n1e-07\n5\naccuracy\nbinary\n0.8622505\n5\n0.0006767\npre0_mod034_post0\n\n\n1e-06\n5\naccuracy\nbinary\n0.8622505\n5\n0.0006767\npre0_mod044_post0\n\n\n1e-05\n5\naccuracy\nbinary\n0.8622505\n5\n0.0006767\npre0_mod054_post0\n\n\n1e-04\n5\naccuracy\nbinary\n0.8622505\n5\n0.0006767\npre0_mod064_post0\n\n\n1e-03\n5\naccuracy\nbinary\n0.8622505\n5\n0.0006767\npre0_mod074_post0\n\n\n1e-04\n15\naccuracy\nbinary\n0.8619633\n5\n0.0005496\npre0_mod070_post0\n\n\n1e-04\n13\naccuracy\nbinary\n0.8618958\n5\n0.0006123\npre0_mod069_post0\n\n\n1e-05\n10\naccuracy\nbinary\n0.8610454\n5\n0.0007958\npre0_mod057_post0\n\n\n0e+00\n10\naccuracy\nbinary\n0.8610116\n5\n0.0007847\npre0_mod007_post0\n\n\n0e+00\n10\naccuracy\nbinary\n0.8610116\n5\n0.0007847\npre0_mod017_post0\n\n\n0e+00\n10\naccuracy\nbinary\n0.8610116\n5\n0.0007847\npre0_mod027_post0\n\n\n1e-07\n10\naccuracy\nbinary\n0.8610116\n5\n0.0007847\npre0_mod037_post0\n\n\n1e-06\n10\naccuracy\nbinary\n0.8610116\n5\n0.0007847\npre0_mod047_post0\n\n\n0e+00\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod001_post0\n\n\n0e+00\n2\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod002_post0\n\n\n0e+00\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod011_post0\n\n\n0e+00\n2\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod012_post0\n\n\n0e+00\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod021_post0\n\n\n0e+00\n2\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod022_post0\n\n\n1e-07\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod031_post0\n\n\n1e-07\n2\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod032_post0\n\n\n1e-06\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod041_post0\n\n\n1e-06\n2\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod042_post0\n\n\n1e-05\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod051_post0\n\n\n1e-05\n2\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod052_post0\n\n\n1e-04\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod061_post0\n\n\n1e-04\n2\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod062_post0\n\n\n1e-03\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod071_post0\n\n\n1e-03\n2\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod072_post0\n\n\n1e-03\n4\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod073_post0\n\n\n1e-02\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod081_post0\n\n\n1e-02\n2\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod082_post0\n\n\n1e-02\n4\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod083_post0\n\n\n1e-02\n5\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod084_post0\n\n\n1e-02\n7\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod085_post0\n\n\n1e-02\n8\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod086_post0\n\n\n1e-02\n10\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod087_post0\n\n\n1e-02\n11\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod088_post0\n\n\n1e-02\n13\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod089_post0\n\n\n1e-02\n15\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod090_post0\n\n\n1e-01\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod091_post0\n\n\n1e-01\n2\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod092_post0\n\n\n1e-01\n4\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod093_post0\n\n\n1e-01\n5\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod094_post0\n\n\n1e-01\n7\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod095_post0\n\n\n1e-01\n8\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod096_post0\n\n\n1e-01\n10\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod097_post0\n\n\n1e-01\n11\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod098_post0\n\n\n1e-01\n13\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod099_post0\n\n\n1e-01\n15\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod100_post0\n\n\n0e+00\n4\naccuracy\nbinary\n0.8604766\n5\n0.0006107\npre0_mod003_post0\n\n\n0e+00\n4\naccuracy\nbinary\n0.8604766\n5\n0.0006107\npre0_mod013_post0\n\n\n0e+00\n4\naccuracy\nbinary\n0.8604766\n5\n0.0006107\npre0_mod023_post0\n\n\n1e-07\n4\naccuracy\nbinary\n0.8604766\n5\n0.0006107\npre0_mod033_post0\n\n\n1e-06\n4\naccuracy\nbinary\n0.8604766\n5\n0.0006107\npre0_mod043_post0\n\n\n1e-05\n4\naccuracy\nbinary\n0.8604766\n5\n0.0006107\npre0_mod053_post0\n\n\n1e-04\n4\naccuracy\nbinary\n0.8604766\n5\n0.0006107\npre0_mod063_post0\n\n\n1e-05\n11\naccuracy\nbinary\n0.8593278\n5\n0.0007112\npre0_mod058_post0\n\n\n0e+00\n11\naccuracy\nbinary\n0.8592265\n5\n0.0007218\npre0_mod008_post0\n\n\n0e+00\n11\naccuracy\nbinary\n0.8592265\n5\n0.0007218\npre0_mod018_post0\n\n\n0e+00\n11\naccuracy\nbinary\n0.8592265\n5\n0.0007218\npre0_mod028_post0\n\n\n1e-07\n11\naccuracy\nbinary\n0.8592265\n5\n0.0007218\npre0_mod038_post0\n\n\n1e-06\n11\naccuracy\nbinary\n0.8592265\n5\n0.0007218\npre0_mod048_post0\n\n\n1e-05\n13\naccuracy\nbinary\n0.8558026\n5\n0.0007028\npre0_mod059_post0\n\n\n0e+00\n13\naccuracy\nbinary\n0.8554534\n5\n0.0006894\npre0_mod009_post0\n\n\n0e+00\n13\naccuracy\nbinary\n0.8554534\n5\n0.0006894\npre0_mod019_post0\n\n\n0e+00\n13\naccuracy\nbinary\n0.8554534\n5\n0.0006894\npre0_mod029_post0\n\n\n1e-07\n13\naccuracy\nbinary\n0.8554534\n5\n0.0006894\npre0_mod039_post0\n\n\n1e-06\n13\naccuracy\nbinary\n0.8554534\n5\n0.0006894\npre0_mod049_post0\n\n\n1e-05\n15\naccuracy\nbinary\n0.8526321\n5\n0.0008015\npre0_mod060_post0\n\n\n0e+00\n15\naccuracy\nbinary\n0.8521084\n5\n0.0007941\npre0_mod010_post0\n\n\n0e+00\n15\naccuracy\nbinary\n0.8521084\n5\n0.0007941\npre0_mod020_post0\n\n\n0e+00\n15\naccuracy\nbinary\n0.8521084\n5\n0.0007941\npre0_mod030_post0\n\n\n1e-07\n15\naccuracy\nbinary\n0.8521084\n5\n0.0007941\npre0_mod040_post0\n\n\n1e-06\n15\naccuracy\nbinary\n0.8521084\n5\n0.0007941\npre0_mod050_post0\n\n\n\n\n\nIt seems we have five models that achieve the minimum cross-validated log loss. These models all have a tree depth of 10, but with different cost complexity values. When we look at accuracy, we see that our best models barely perform above the no-information rate of 0.861. That is, we only slightly outperform the strategy of predicting that no one has diabetes. This does not bode well for our ability to effectively predict diabetes incidence.\nLet’s now extract our best model based on log loss. In particular, let’s select the simplest model (largest cost complexity) among the five top-performing models.\n\n#Capturing best tree model (least complex \"best\" model)\ntree_best_params&lt;-show_best(tree_fit, metric = \"mn_log_loss\", n = Inf) |&gt;\n  filter(mean==min(mean)) |&gt;\n  slice_max(cost_complexity) |&gt;\n  select(cost_complexity, tree_depth)\n\n\n\n#Printing hyperparameter value\ntree_best_params\n\n# A tibble: 1 × 2\n  cost_complexity tree_depth\n            &lt;dbl&gt;      &lt;int&gt;\n1        0.000001         10\n\n\nNow that we have identified our best classification tree model, let’s tune the random forest.\n\n\nTuning the Random Forest\nRandom forests are explicitly designed to overcome a key drawback of a single tree, such as we have in the classification tree model. This limitation is that single trees are highly sensitive to changes in their training data, making their fits highly volatile. Random forests solve this issue by truly creating a random forest with hundreds of trees. We then combine predictions across the trees to generate a single, more stable prediction.\nTo create these hundreds of random trees, the model repeated draws non-parametric bootstrap samples from the training data that are the same size as the original dataset and fits a tree to each sample. This naturally creates variability across trees. If you are familiar with the bagged tree model, you are likely thinking this sounds just like that model. You are correct that we create a forest via a bootstrap sampling mechanism for each model. However, there is one clear distinction between the two models. At each split, random forests take a random sample of the predictors before selecting the predictor to split by; the bagged tree model can be thought of as a special case of the random forest where the number of predictors we “randomly select” for each split is equal to the total number of predictors. This ensures we are not allowing a few dominant predictors determine the most important splits for every tree. This is very important because creating more distinct trees will reduce the variability of our predictions in comparison to an ensemble model with highly “correlated” sub-models.\nTo tune the random forest model, we will try different values of mtry, or the number of predictors randomly selected for each split. We will set the minimum number of observation in any endpoint to 20, and we will also set the total number of trees in each random forest to 500. This creates relatively small random forests but ones that are more computationally manageable.\nNow that we understand the random forest and how we will tune it, let’s specify our workflow. Note that we have now changed our engine to the popular ranger engine and have indicated we want to tune mtry.\n\n#Specifying model and engine for random forest\nrf_model&lt;-rand_forest(mtry = tune(),\n                          min_n = 20,\n                          trees = 500) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\", seed = 25) |&gt;\n  set_mode(\"classification\")\n\n#Creating random forest workflow\nrf_wkf&lt;-workflow() |&gt;\n  add_recipe(recipe) |&gt;\n  add_model(rf_model)\n\nNow that we have our workflow, we are ready to fit our models. We are trying all possible values of mtry given that we have 11 predictors (1 to 11). This results in 55 total models fit given that we are utilizing 5-fold CV.\n\n#Fitting random forest model across values of mtry\nrf_fit&lt;-rf_wkf |&gt;\n  tune_grid(resamples = folds,\n            grid = grid_regular(mtry(range = c(1,11)), levels = 11),\n            metrics = metric_set(mn_log_loss, accuracy)) \n\nNow that we have fit these 55 models, we will extract average log-loss for each value of mtry. As we did for the classification tree model, we will also extract average accuracy values for a prediction metric that is easier to interpret.\n\n#Sorting models by log loss\nrf_fit |&gt; collect_metrics() |&gt; filter(.metric==\"mn_log_loss\") |&gt;\n  arrange(mean) |&gt;\n  kable()\n\nWarning: 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\nWarning: 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmtry\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n3\nmn_log_loss\nbinary\n0.3295532\n5\n0.0009553\npre0_mod03_post0\n\n\n2\nmn_log_loss\nbinary\n0.3307841\n5\n0.0010052\npre0_mod02_post0\n\n\n4\nmn_log_loss\nbinary\n0.3339265\n5\n0.0010409\npre0_mod04_post0\n\n\n5\nmn_log_loss\nbinary\n0.3392282\n5\n0.0011195\npre0_mod05_post0\n\n\n6\nmn_log_loss\nbinary\n0.3435348\n5\n0.0012359\npre0_mod06_post0\n\n\n7\nmn_log_loss\nbinary\n0.3491324\n5\n0.0015523\npre0_mod07_post0\n\n\n1\nmn_log_loss\nbinary\n0.3514497\n5\n0.0008205\npre0_mod01_post0\n\n\n8\nmn_log_loss\nbinary\n0.3544302\n5\n0.0016641\npre0_mod08_post0\n\n\n9\nmn_log_loss\nbinary\n0.3606347\n5\n0.0023199\npre0_mod09_post0\n\n\n10\nmn_log_loss\nbinary\n0.3706848\n5\n0.0023809\npre0_mod10_post0\n\n\n11\nmn_log_loss\nbinary\n0.3868094\n5\n0.0042066\npre0_mod11_post0\n\n\n\n\n#Sorting models by accuracy\nrf_fit |&gt; collect_metrics() |&gt; filter(.metric==\"accuracy\") |&gt;\n  arrange(desc(mean)) |&gt;\n  kable()\n\nWarning: 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\nWarning: 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmtry\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n3\naccuracy\nbinary\n0.8635063\n5\n0.0006229\npre0_mod03_post0\n\n\n4\naccuracy\nbinary\n0.8628306\n5\n0.0006057\npre0_mod04_post0\n\n\n2\naccuracy\nbinary\n0.8621379\n5\n0.0005701\npre0_mod02_post0\n\n\n5\naccuracy\nbinary\n0.8619971\n5\n0.0006249\npre0_mod05_post0\n\n\n6\naccuracy\nbinary\n0.8614903\n5\n0.0005056\npre0_mod06_post0\n\n\n7\naccuracy\nbinary\n0.8608202\n5\n0.0007122\npre0_mod07_post0\n\n\n1\naccuracy\nbinary\n0.8607075\n5\n0.0005507\npre0_mod01_post0\n\n\n8\naccuracy\nbinary\n0.8602401\n5\n0.0006701\npre0_mod08_post0\n\n\n9\naccuracy\nbinary\n0.8598234\n5\n0.0007144\npre0_mod09_post0\n\n\n10\naccuracy\nbinary\n0.8596207\n5\n0.0005799\npre0_mod10_post0\n\n\n11\naccuracy\nbinary\n0.8591195\n5\n0.0004818\npre0_mod11_post0\n\n\n\n\n\nIn this case, the same model had the best cross-validated log loss and accuracy: the model that randomly selects three predictors to use at each split.\nFocusing on the accuracy values, we again see that our accuracy is barely above the no-information rate. It seems we have not found great predictors of diabetes incidence!\nAs a final step in the random forest tuning process, let’s extract the best mtry value (3) so that we can fit a random forest to the entire training set using that value.\n\n#Capturing best random forest model\nrf_best_params&lt;-select_best(rf_fit, metric = \"mn_log_loss\")\n\n#Printing hyperparameter values\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config         \n  &lt;int&gt; &lt;chr&gt;           \n1     3 pre0_mod03_post0\n\n\nNow that we have captured the best-performing mtry value, we are ready to compare our best classification tree and random forest models.\n#Comparing Performance on Test Set\nLet’s now fit our best classification tree and random forest models to the entire training set and see how well they predict on our full test set. We will start with the classification tree. Note that the last_fit() function both fits the workflow-specified model to the full training set and tests on the test set all in one line of code. Truly tidy!\n\n#Setting classification tree hyperparameter values to those selected via CV\ntree_final_wkf&lt;-tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\n#Fitting final model to the full training set and testing on original test set\ntree_final_fit&lt;-tree_final_wkf |&gt;\n  last_fit(split, metrics = metric_set(mn_log_loss, accuracy))\n\nNow that we have fit and tested the best classification tree, we will complete the same process for the best random forest model.\n\n#Setting rf hyperparameter values to those selected via CV\nrf_final_wkf&lt;-rf_wkf |&gt;\n  finalize_workflow(rf_best_params)\n\n#Fitting final model to the full training set and testing on original test set\nrf_final_fit&lt;-rf_final_wkf |&gt;\n  last_fit(split, metrics = metric_set(mn_log_loss, accuracy))\n\nWe have now fit both models to the training set and predicted on the test set. Let’s extract the test metrics to see how the models performed and select a final model.\n\n#Printing log loss and accuracy for the best models of each class\nrbind( tree_final_fit |&gt; collect_metrics() |&gt; mutate(model = \"Class Tree\") |&gt; select(model, everything()),\n      rf_final_fit |&gt; collect_metrics() |&gt; mutate(model = \"Random Forest\") |&gt; select(model, everything()))\n\n# A tibble: 4 × 5\n  model         .metric     .estimator .estimate .config        \n  &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 Class Tree    accuracy    binary         0.860 pre0_mod0_post0\n2 Class Tree    mn_log_loss binary         0.343 pre0_mod0_post0\n3 Random Forest accuracy    binary         0.863 pre0_mod0_post0\n4 Random Forest mn_log_loss binary         0.332 pre0_mod0_post0\n\n\nOur best random forest model outperforms our best classification tree in terms of both accuracy and log loss. Therefore, we will select this tuned random forest model as our final model and fit it to the entire dataset before taking it to production.\n\n\nFitting and Exploring the Final Model\nNow that we have selected a final model, let’s fit it to the full dataset so that we can learn about it and make some initial predictions.\n\n#Fitting the final random forest model to the full dataset\nrf_production_fit&lt;-rf_final_wkf |&gt;\n  fit(data)\n\nTo better understand why our final model likely won’t predict was well as we would have hoped, let’s produce the confusion matrix for our final model fit to the full dataset; this is a training set confusion matrix, so it will be informative but optimistic.\n\n#Generating predictions for the full dataset\npredictions&lt;-predict(rf_production_fit,new_data = data)$.pred_class\n\n#Creating confusion matrix\nautoplot(conf_mat(data |&gt; mutate(prediction = predictions), diabetes_binary, prediction), type = \"heatmap\")\n\n\n\n\n\n\n\n\nLooking at this confusion matrix, it is clear our model struggles to identify those with diabetes and, therefore, often defaults to the most prevalent class (no diabetes). This results in over 31,141 individuals in our dataset misidentified as individuals without diabetes. Meanwhile, our model predicts an individual does have diabetes less than 6,000 times for a dataset with 35,346 individuals who actually have diabetes.\nNow that we have fit the final model and explored the training confusion matrix, we can use it to make predictions. For example, the code below predicts whether or not an individual with the following characteristics has diabetes: - Has not exercised in the past month - Has high blood pressure - Has high cholesterol - Does not eat their fruits and vegetables - Has a history of smoking - Is not a heavy drinker - Is male - Is between the ages of 60 and 64 - Earns between $15,000 and $20,000 per year - Has a BMI of 40\n\n#Creating a new observation to predict for using our final model\nnew_obs&lt;-tibble(.rows = 1) |&gt;\n  mutate(phys_activity = factor(0, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(high_bp = factor(1, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(high_chol = factor(1, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(fruits = factor(0, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(veggies = factor(0, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(smoker = factor(1, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(hvy_alc_consump = factor(0, levels = 0:1, labels = c(\"No\", \"Yes\"))) |&gt;\n  mutate(sex = factor(1, levels = 0:1, labels = c(\"Female\", \"Male\"))) |&gt;\n  mutate(age = factor(9, levels = 1:13, labels = c(\"18 to 24\", \"25 to 29\",\n                                                   \"30 to 34\", \"35 to 39\",\n                                                   \"40 to 44\", \"45 to 49\",\n                                                   \"50 to 54\", \"55 to 59\",\n                                                   \"60 to 64\", \"65 to 69\",\n                                                   \"70 to 74\", \"75 to 79\",\n                                                   \"80+\"))) |&gt;\n  mutate(income = factor(3, levels = 1:8, labels = c(\"Less than $10k\", \"$10k to $15k\",\n                                                   \"$15k to $20k\", \"$20k to $25k\",\n                                                   \"$25k to $35k\", \"$35k to $50k\",\n                                                   \"$50k to $75k\", \"$75k+\"))) |&gt;\n  mutate(bmi = 40)\n\n#Predicting whether or not this new individual has diabetes\npredict(rf_production_fit, new_data = new_obs)$.pred_class\n\n[1] Yes\nLevels: No Yes\n\n\nIt seems this individual is one of the rare people our model predicts to have diabetes.\n\n\nWrap-Up\nThis document has described our process of tuning both the classification tree and random forest models to predict whether or not an individual has diabetes. It seems the factors we have explored are not particularly useful for predicting diabetes, or at least not in combination with the two model classes we have tested.\nWhile not permanently hosted on the web, I have also produced an API that can be used to extract predictions from our final model fit to the full dataset, and the code for this API lives in the associated GitHub repo. If you deploy this API, happy predicting!"
  }
]